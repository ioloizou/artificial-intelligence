{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-98T1e0Fo-_"
      },
      "source": [
        "# Introduction to Convolutional Neural Networks\n",
        "\n",
        "\n",
        "** Ecole Centrale Nantes **\n",
        "\n",
        "** Diana Mateus **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWvMyUuIFo_F"
      },
      "source": [
        "** Participants : **\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BbzD2vUXFo_J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sO2dQfUFo_L"
      },
      "source": [
        "### 0. Loading the dataset\n",
        "Start by runing the following lines to load and visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnoNobk1KaPM"
      },
      "outputs": [],
      "source": [
        "# UNCOMMENT IF USING COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "IMDIR = '/content/drive/MyDrive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMhp19cHFo_M"
      },
      "outputs": [],
      "source": [
        "def load_dataset(IMDIR):\n",
        "    train_dataset = h5py.File(IMDIR+'dataset/train_catvnoncat.h5', \"r\")\n",
        "    train_x = np.array(train_dataset[\"train_set_x\"][:])\n",
        "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "    test_dataset = h5py.File(IMDIR+'dataset/test_catvnoncat.h5', \"r\")\n",
        "    test_x = np.array(test_dataset[\"test_set_x\"][:])\n",
        "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
        "\n",
        "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
        "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, classes\n",
        "\n",
        "train_x, train_y, test_x, test_y, classes=load_dataset(IMDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKTEbV8Fo_O"
      },
      "source": [
        "#### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY74AXWEFo_O"
      },
      "outputs": [],
      "source": [
        "# run several times to visualize different data points\n",
        "# the title shows the ground truth class labels (0=no cat , 1 = cat)\n",
        "index = np.random.randint(low=0,high=train_y.shape[1])\n",
        "plt.imshow(train_x[index])\n",
        "plt.title(\"Image \"+str(index)+\" label \"+str(train_y[0,index]))\n",
        "plt.show()\n",
        "print (\"Train X shape: \" + str(train_x.shape))\n",
        "print (\"We have \"+str(train_x.shape[0]),\n",
        "       \"images of dimensionality \"\n",
        "       + str(train_x.shape[1])+ \"x\"\n",
        "       + str(train_x.shape[2])+ \"x\"\n",
        "       + str(train_x.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SWukehhFo_g"
      },
      "source": [
        "### 1. CNNs with Keras and Tensorflow\n",
        "\n",
        "Adapt the example in this website https://keras.io/examples/vision/mnist_convnet/ to our problem. To this end:\n",
        "- change the number of classes and the input size\n",
        "- remove the expand_dims(x_train, -1): it is not necessary to expand the dimensions since our input has already three channels\n",
        "- you may need to transpose the labels vector\n",
        "- change the categorical cross-entropy to the binary cross entropy given that our problem is binary classification.\n",
        "- also change the softmax to sigmoid, the more appropriate activation function for binary data\n",
        "\n",
        "We can choose a single neuron output passed through sigmoid, and then set a threshold to choose the class, or use two neuron output and then perform a softmax.\n",
        "\n",
        "**2.2** Compute the train and test loss and accuracy after the model has been trained.  What model parameters does the ``fit`` function retain?\n",
        "\n",
        "**2.3** How many parameters does the network have, explain  the exact number .\n",
        "\n",
        "**2.4** Display and discuss the ROC curve of at least 3 different CNN configurations  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhzjC8_VFo_h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv4lWad0Fo_j"
      },
      "outputs": [],
      "source": [
        "# the data, split between train and test sets\n",
        "x_train, y_train, x_test, y_test, classes=load_dataset(IMDIR)\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "\n",
        "num_classes = 2\n",
        "input_shape = (64, 64, 3)\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhgPycoVFo_l"
      },
      "outputs": [],
      "source": [
        "#build the model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape= input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation = \"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmO5teIr5Nb5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL0w6OeFFo_o"
      },
      "outputs": [],
      "source": [
        "#comiple and fit\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "pred_original = model.predict(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LX_FTgzFo_p"
      },
      "outputs": [],
      "source": [
        "#evaluate\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "\n",
        "#2.1 The model is retaining all the weights and the biases\n",
        "\n",
        "#2.2 It has 44482 parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yxsy7d-2u5g"
      },
      "outputs": [],
      "source": [
        "def wider_model():\n",
        "  #create the model\n",
        "  model_wider = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape= input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(64, kernel_size=(5, 5), activation = \"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  #comiple and fit\n",
        "  batch_size = 32\n",
        "  epochs = 15\n",
        "\n",
        "  model_wider.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  model_wider.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  #evaluate\n",
        "  score = model_wider.evaluate(x_test, y_test, verbose=0)\n",
        "  print(\"Test loss:\", score[0])\n",
        "  print(\"Test accuracy:\", score[1])\n",
        "  y_pred = model_wider.predict(x_test)\n",
        "\n",
        "  return y_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN9u8w1N9ES-"
      },
      "outputs": [],
      "source": [
        "pred_wider = wider_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t9afLUnNiXt"
      },
      "outputs": [],
      "source": [
        "def deeper_model():\n",
        "  #create the model\n",
        "  model_deeper = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape= input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation = \"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3), activation = \"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  #comiple and fit\n",
        "  batch_size = 32\n",
        "  epochs = 15\n",
        "\n",
        "  model_deeper.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  model_deeper.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  #evaluate\n",
        "  score = model_deeper.evaluate(x_test, y_test, verbose=0)\n",
        "  print(\"Test loss:\", score[0])\n",
        "  print(\"Test accuracy:\", score[1])\n",
        "  y_pred = model_deeper.predict(x_test)\n",
        "  plot_model(model_deeper, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "  return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-pbNzR4NxzE"
      },
      "outputs": [],
      "source": [
        "pred_deeper = deeper_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwUEfUd9U3eD"
      },
      "outputs": [],
      "source": [
        "def no_pooling_model():\n",
        "  #create the model\n",
        "  model_no_pooling = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape= input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation = \"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  #comiple and fit\n",
        "  batch_size = 32\n",
        "  epochs = 15\n",
        "\n",
        "  model_no_pooling.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  model_no_pooling.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "\n",
        "\n",
        "  #evaluate\n",
        "  score = model_no_pooling.evaluate(x_test, y_test, verbose=0)\n",
        "  print(\"Test loss:\", score[0])\n",
        "  print(\"Test accuracy:\", score[1])\n",
        "\n",
        "  y_pred = model_no_pooling.predict(x_test)\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m69onIumU3xQ"
      },
      "outputs": [],
      "source": [
        "pred_no_pooling = no_pooling_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akujxI6p2ZEE"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "predictions = [pred_original, pred_wider, pred_deeper, pred_no_pooling]\n",
        "names = ['Original', 'Wider', 'Deeper', 'No pooling']\n",
        "i = 0\n",
        "binary_labels = y_test[:, 1]\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for prediction in predictions:\n",
        "  y_pred = prediction[:, 1]\n",
        "\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(binary_labels, y_pred)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "  # Plot ROC curve\n",
        "  plt.plot(fpr, tpr, lw=2, label= names[i] + ' (area = {:.2f})'.format(roc_auc))\n",
        "  i+= 1;\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX_LrVY_Fo_q"
      },
      "source": [
        "## 2 Custom training loop (OPTIONAL ARTIN)\n",
        "Replace the fit function by your own tensorflow  implementation\n",
        "\n",
        "- Instantiate one of keras.optimizers to train the model.\n",
        "\n",
        "- Instantiate a loss from keras.losses\n",
        "\n",
        "- Define the metrics (from keras.metrics)\n",
        "\n",
        "- Use `tf.data.Dataset.from_tensor_slices` to create an iterable dataset from a numpy arrays. Do this for the training and test datasets.\n",
        "\n",
        "- Change the model (optional, after the training loop runs to optimize the performance)\n",
        "\n",
        "- Program a loop over a fixed number of epochs,\n",
        "    * For each epoch iterating over the batches\n",
        "    * Within a `GradientTape()` scope,\n",
        "      - do a forward pass on the model for the current batch (call the model on the batch data)\n",
        "      - Compute the loss\n",
        "      - Compute the gradients of the loss w.r.t parameters\n",
        "      - Call the optimimzer to update the weights with computed the gradients\n",
        "    * At the end of each epoch compute the validation metrics\n",
        "\n",
        "\n",
        "Look at https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough specifically at the TRAINING LOOP SECTION\n",
        "for a recent documentation on custom training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMeFjNH64CWD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "\n",
        "def manual_fit():\n",
        "  model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape= input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation = \"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  #comiple and fit\n",
        "  batch_size = 32\n",
        "  epochs = 15\n",
        "\n",
        "  optimizer = Adam(learning_rate = 0.001)\n",
        "  loss_fn = BinaryCrossentropy()\n",
        "  accuracy_metric = BinaryAccuracy()\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            logits = model(batch_x)\n",
        "\n",
        "            loss_value = loss_fn(batch_y, logits)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        # Update weights\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        # Update accuracy metric\n",
        "        accuracy_metric.update_state(batch_y, logits)\n",
        "\n",
        "    # Compute validation metrics at the end of each epoch\n",
        "    for batch_x_test, batch_y_test in test_dataset:\n",
        "        logits_test = model(batch_x_test)\n",
        "        accuracy_metric.update_state(batch_y_test, logits_test)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {loss_value:.4f}, Accuracy = {accuracy_metric.result().numpy():.4f}\")\n",
        "    accuracy_metric.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84DHHgYabJpk"
      },
      "outputs": [],
      "source": [
        "manual_fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd77rWJY4Eon"
      },
      "source": [
        "ADITIONAL BONUS\n",
        "- Early stopping\n",
        "- Tensorboard\n",
        "- CAM/GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-PQ71UOFo_r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "import datetime\n",
        "\n",
        "def deeper_model_with_callbacks():\n",
        "    model_deeper = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "            layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Dropout(0.4),\n",
        "            layers.Flatten(),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(num_classes, activation=\"sigmoid\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Compile and fit with EarlyStopping and TensorBoard callbacks\n",
        "    batch_size = 32\n",
        "    epochs = 15\n",
        "\n",
        "    model_deeper.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "    model_deeper.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, tensorboard]\n",
        "    )\n",
        "\n",
        "    score = model_deeper.evaluate(x_test, y_test, verbose=0)\n",
        "    print(\"Test loss:\", score[0])\n",
        "    print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujI6H5cQ7nms"
      },
      "outputs": [],
      "source": [
        "deeper_model_with_callbacks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkjxYZ_J7rH6"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz61ROZ5oNQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_size = (64, 64)\n",
        "\n",
        "# Assuming train_x[0] is your example image\n",
        "img_array = train_x[156]\n",
        "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "print(img_array.shape)\n",
        "# Choose the last convolutional layer name\n",
        "last_conv_layer_name = \"conv2d_1\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdyxtxxxzSlT"
      },
      "outputs": [],
      "source": [
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmLa4nNfzUbX"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_array = preprocess_input(img_array)\n",
        "print(img_array.shape)\n",
        "\n",
        "\n",
        "# Make model\n",
        "model = model\n",
        "\n",
        "# Remove last layer's softmax\n",
        "model.layers[-1].activation = None\n",
        "\n",
        "# Print what the top predicted class is\n",
        "# preds = model.predict(img_array)\n",
        "# print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf2",
      "language": "python",
      "name": "tf2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}